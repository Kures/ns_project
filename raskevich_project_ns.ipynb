{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "raskevich_project_ns.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "891eUnBBAyD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "e51e211a-dc9f-48e8-a659-3e1a04efa6e7"
      },
      "source": [
        "!pip install dnspython\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dnspython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d3/3aa0e7213ef72b8585747aa0e271a9523e713813b9a20177ebe1e939deb0/dnspython-1.16.0-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-1.16.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMkvCB3nTCbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests, re\n",
        "import bs4\n",
        "import lxml.etree as xml\n",
        "\n",
        "import dns\n",
        "import pymongo as pm\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI_OX3qyTNIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d38227a0-066b-4dbb-a646-42046fbe4b32"
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (91.189.88.31)] [Connecting to cloud.r-pr\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [2 InRelease 5,500 B/88.7 kB 6%] [Connecting to security.ubuntu.com (91.189.\r0% [1 InRelease gpgv 242 kB] [2 InRelease 8,396 B/88.7 kB 9%] [Connecting to se\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Conne\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r0% [1 InRelease gpgv 242 kB] [3 InRelease 4,052 B/74.6 kB 5%] [Waiting for head\r                                                                               \rHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:6 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [879 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,232 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,927 B]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,648 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [720 kB]\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [58.4 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [570 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [791 kB]\n",
            "Err:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "  502  Bad Gateway [IP: 192.229.211.70 443]\n",
            "Ign:17 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:18 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:19 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [801 B]\n",
            "Get:20 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [10.8 kB]\n",
            "Fetched 6,185 kB in 6s (1,045 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Failed to fetch https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/InRelease  502  Bad Gateway [IP: 192.229.211.70 443]\n",
            "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 70.1 MB of archives.\n",
            "After this operation, 257 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 75.0.3770.90-0ubuntu0.18.04.1 [1,112 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 75.0.3770.90-0ubuntu0.18.04.1 [61.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 75.0.3770.90-0ubuntu0.18.04.1 [2,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 75.0.3770.90-0ubuntu0.18.04.1 [4,482 kB]\n",
            "Fetched 70.1 MB in 1s (58.8 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 130942 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_75.0.3770.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_75.0.3770.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_75.0.3770.90-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_75.0.3770.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Setting up chromium-browser (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (75.0.3770.90-0ubuntu0.18.04.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQznZpg6TCeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set options to be headless\n",
        "from selenium import webdriver\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbBEmlSiTvhc",
        "colab_type": "text"
      },
      "source": [
        "For scopus we need author ID to get their history.\n",
        "\n",
        "For example: \n",
        "https://proxylibrary.hse.ru:2073/author/historydetails.uri?authorId=36787804000 \n",
        "We get this author ID by searching different article in scopus API. We have this data in our online MongoDB database.\n",
        "So, we can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldriK9i4TGX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the link of our online Database.\n",
        "url4 = \"mongodb+srv://mydb:mydb4@cluster0-65voy.gcp.mongodb.net/hsensfour?retryWrites=true\"\n",
        "client4 = pm.MongoClient(url4)\n",
        "\n",
        "db = client4.hsensfour\n",
        "authinfo = db.mysite_scoinfo\n",
        "\n",
        "# All information about the author is present in this \"abc\" variable.\n",
        "abc = authinfo.find({})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElvSQy-DTGaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2f14979a-ff98-4c3d-ca27-da8884d3c84e"
      },
      "source": [
        "import time\n",
        "\n",
        "wd = webdriver.Chrome('chromedriver',options=options)\n",
        "wd.close()\n",
        "wd = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "allresults = []\n",
        "first = True\n",
        "index = 0\n",
        "for auth in abc:\n",
        "  uid = auth['ID']\n",
        "  print(uid)\n",
        "  \n",
        "  # If this is first time then we need to log in. Otherwise we can parse easily without log in. Because we are already loged in.\n",
        "  if first:\n",
        "    first = False\n",
        "    wd.get(\"https://proxylibrary.hse.ru:2073/author/historydetails.uri?authorId=\"+str(uid))\n",
        "    wd.find_element_by_name(\"user\").send_keys(\"oomatkarimov@edu.hse.ru\")\n",
        "    wd.find_element_by_name(\"pass\").send_keys(\"ZKwgKQYpy\")\n",
        "    wd.find_element_by_xpath(\"/html/body/center/table/tbody/tr/td/div[2]/form/input[3]\").click()\n",
        "    wd.get(\"https://proxylibrary.hse.ru:2073/author/historydetails.uri?authorId=\"+str(uid))\n",
        "  else: \n",
        "    # This is the history page.\n",
        "    wd.get(\"https://proxylibrary.hse.ru:2073/author/historydetails.uri?authorId=\"+str(uid))\n",
        "  \n",
        "  # After every valid request we are waiting 2 sec. It will help us to bypass robot status and Scopus will think we are using real browser.\n",
        "  time.sleep(2)\n",
        "  # Now we have the result HTML page. It's time to parse it.\n",
        "  total = wd.page_source\n",
        "  total = bs4.BeautifulSoup(total, \"lxml\")\n",
        "  organlist = []\n",
        "  try:\n",
        "    alltitle = total.find_all(name=\"tbody\", attrs={\"class\": \"borderBottom2\"})[1]\n",
        "    organ = alltitle.find_all(name=\"span\", attrs={\"class\": \"anchorText\"})\n",
        "    for orname in organ:\n",
        "      organlist.append(orname.text.strip())\n",
        "    organ = alltitle.find_all(name=\"td\", attrs={\"class\": \"col-md-4\"})\n",
        "    for orname in organ:\n",
        "      organlist.append(orname.text.strip())\n",
        "  except:\n",
        "     print(\"Something is wrong. Maybe this user don't have much affiliations..\")\n",
        "  \n",
        "  # Use it to remove duplicate organization name.\n",
        "  organlist = set(organlist)\n",
        "  organlist = list(organlist)\n",
        "  \n",
        "  # This part is only for make it json format.\n",
        "  class BaseRespone():\n",
        "    def __init__(self):\n",
        "      self.ID = int(uid)\n",
        "      self.name = auth['name']\n",
        "      self.organisation = organlist\n",
        "\n",
        "  def obj_to_dict(obj):\n",
        "    return obj.__dict__\n",
        "  \n",
        "  bs = BaseRespone()\n",
        "  json_string = json.dumps(bs.__dict__, default = obj_to_dict)\n",
        "  final_result = json.loads(json_string)\n",
        "  allresults.append(final_result)\n",
        "    \n",
        "  # Point for exit. If comment it - will run for all id authors\n",
        "  if index % 500 == 0:\n",
        "    print(f\"Current index = {index}\")\n",
        "  if index == 12000:\n",
        "    break\n",
        "  index += 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57200597517\n",
            "Current index = 0\n",
            "9335162000\n",
            "36895078500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a1JIb8PU2wH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "9b8b9e73-5a1c-4bcc-e131-215403d0789e"
      },
      "source": [
        "#All Results.\n",
        "print(allresults)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'ID': 57200597517, 'name': 'Gebrie, Anteneh Getachew', 'organisation': ['Naresuan University']}, {'ID': 9335162000, 'name': 'Wangkeeree, Rabian', 'organisation': ['South Carolina Commission on Higher Education', 'Centre of Excellence in Mathematics', 'Naresuan University']}, {'ID': 36895078500, 'name': 'Qian, Weimao', 'organisation': ['School of Continuing Education, Huzhou Vocational and Technical College', 'Huzhou Broadcast and TV University', 'TV University', 'Huzhou Vocational & Technical College', 'Hunan City University', 'School of Continuing Education, Huzhou Vocational and Technological College']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0DxRfTTTGdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvs8ch8idAqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "ec12b002-3cac-4342-deae-6ac0c5556714"
      },
      "source": [
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('drive/My Drive/')\n",
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            " AAPL.csv\t       ignatov_task_04\t       otabek\n",
            " all_articles.json     kaggle\t\t       Raskevich.rar\n",
            " all_authors_id.json   lab_2\t\t       sophia_hacker_23.gdoc\n",
            " ASL\t\t       LoLcode\t\t       test_unity_02\n",
            " auth-org.json\t       mnist_conv.ipynb        Unity\n",
            "'Colab Notebooks'      mnist_conv_test.ipynb   utils.py\n",
            " data\t\t       Notability\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVp-F_VPTLD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"auth-org.json\",\"w\") as f:\n",
        "  f.write(str(allresults))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
